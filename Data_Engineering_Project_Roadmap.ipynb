{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "078fa8eb",
   "metadata": {},
   "source": [
    "# Data Engineering Project Roadmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446a50d7",
   "metadata": {},
   "source": [
    "### Project Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0f2a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# airflow_project/\n",
    "# │\n",
    "# ├── dags/                     # Airflow DAGs directory\n",
    "# │   ├── noaa_etl_dag.py       # Main DAG for the NOAA ETL pipeline\n",
    "# │   ├── __init__.py\n",
    "# │   └── scripts/              # Scripts used by the DAG\n",
    "# │       ├── extract_noaa_data.py   # Data extraction script\n",
    "# │       ├── transform_data.py      # PySpark data transformation script\n",
    "# │       └── upload_to_s3.py        # Upload data to S3 script\n",
    "# │\n",
    "# ├── data/                     # Data storage (temporary or final)\n",
    "# │   ├── raw/                  # Raw data from NOAA APIs\n",
    "# │   │   └── noaa_data.json\n",
    "# │   ├── transformed/          # Transformed data ready for Tableau\n",
    "# │   │   └── transformed_data.csv\n",
    "# │   └── postgres_backup/      # Optional: PostgreSQL backups (if using PostgreSQL)\n",
    "# │\n",
    "# ├── docker-compose.yaml       # Docker Compose config for Airflow and PostgreSQL\n",
    "# ├── Dockerfile                # Optional: Dockerfile for customizing the Airflow container\n",
    "# ├── venv/                     # Virtual environment directory\n",
    "# ├── requirements.txt          # Python dependencies\n",
    "# ├── .gitignore                # Ignore unnecessary files for Git\n",
    "# └── README.md                 # Project documentation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ad8587",
   "metadata": {},
   "source": [
    "## Phase 1: Project Setup and Environment Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a82669",
   "metadata": {},
   "source": [
    "\n",
    "### Goal: Set up the tools, environment, and infrastructure needed to run your project.\n",
    "\n",
    "1. **Set Up AWS Resources:**\n",
    "   - **S3 Buckets**:\n",
    "     - Create two S3 buckets: \n",
    "       - **Raw Data Storage** (e.g., `s3://your-bucket/raw-data/`)\n",
    "       - **Processed Data Storage** (e.g., `s3://your-bucket/processed-data/`)\n",
    "   - **IAM Roles and Policies**:\n",
    "     - Create an **IAM Role** with:\n",
    "       - **S3 read/write access**\n",
    "       - **Glue and Redshift permissions**\n",
    "     - Attach the role to your **Glue jobs** and **Redshift** COPY operations.\n",
    "\n",
    "2. **Set Up PostgreSQL (Optional for Metadata Storage)**:\n",
    "   - Install PostgreSQL or use **Amazon RDS** to store metadata or staging data.\n",
    "\n",
    "3. **Install and Configure Airflow**:\n",
    "   - Use **Docker** or **Local Setup** to install Airflow:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b13e312",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install apache-airflow boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbcb358",
   "metadata": {},
   "source": [
    "\n",
    "4. **Install Required Packages**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d52b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install boto3 pandas sqlalchemy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68db752b",
   "metadata": {},
   "source": [
    "## Phase 2: Data Extraction and Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4456e87b",
   "metadata": {},
   "source": [
    "\n",
    "### Goal: Ingest data into S3 and set up the initial pipeline to load data into PostgreSQL and Redshift.\n",
    "\n",
    "Below is a Python code example for extracting data from S3 using **Boto3**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dfc955",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import boto3\n",
    "\n",
    "def extract_data_from_s3():\n",
    "    # Download data from S3 to local storage.\n",
    "    s3 = boto3.client('s3')\n",
    "    s3.download_file('noaa-nexrad-level2', '2022/01/01/sample.csv', './data/sample.csv')\n",
    "    print(\"Data downloaded from S3.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb5263b",
   "metadata": {},
   "source": [
    "## Phase 3: AWS Glue Development and ETL Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11388dc6",
   "metadata": {},
   "source": [
    "\n",
    "### Goal: Set up AWS Glue for data transformation and write PySpark code for ETL tasks.\n",
    "\n",
    "Below is a PySpark code example for temperature conversion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5107d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"NOAA_ETL\").getOrCreate()\n",
    "df = spark.read.csv(\"s3://your-bucket/raw-data/sample.csv\", header=True, inferSchema=True)\n",
    "\n",
    "df_transformed = df.withColumn(\n",
    "    'temp_celsius', (df['temperature'] - 32) * 5.0 / 9.0\n",
    ")\n",
    "df_transformed.write.parquet(\"s3://your-bucket/processed-data/\")\n",
    "print(\"Data transformed and written to S3.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56265bfe",
   "metadata": {},
   "source": [
    "## Phase 4: Data Transformation and Redshift Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e69e38",
   "metadata": {},
   "source": [
    "\n",
    "### Goal: Write transformed data to Redshift and create analytical tables for Tableau.\n",
    "\n",
    "Below is the SQL COPY command for loading data from S3 to Redshift.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6df7bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "COPY noaa_weather\n",
    "FROM 's3://your-bucket/processed-data/'\n",
    "IAM_ROLE 'arn:aws:iam::your-account-id:role/RedshiftCopyRole'\n",
    "FORMAT AS PARQUET;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6e0bef",
   "metadata": {},
   "source": [
    "## Phase 5: Data Validation and Monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6945259",
   "metadata": {},
   "source": [
    "\n",
    "### Goal: Automate data validation and error handling for your pipeline.\n",
    "\n",
    "Example Python code for data validation using Airflow:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2beb6f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "def validate_data():\n",
    "    df = pd.read_csv('./data/sample.csv')\n",
    "    missing_count = df['temperature'].isnull().sum()\n",
    "    if missing_count > 0:\n",
    "        raise ValueError(f\"{missing_count} missing values found!\")\n",
    "    print(\"Data validation passed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3b11a4",
   "metadata": {},
   "source": [
    "## Phase 6: LLM Integration for Documentation and Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5a8385",
   "metadata": {},
   "source": [
    "\n",
    "Below is a Python function using OpenAI API to generate data documentation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f0fea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import openai\n",
    "\n",
    "def generate_documentation():\n",
    "    response = openai.Completion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        prompt=\"Generate documentation for the NOAA dataset...\",\n",
    "        max_tokens=200\n",
    "    )\n",
    "    print(response.choices[0].text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4070e26",
   "metadata": {},
   "source": [
    "## Phase 7: Tableau Integration and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20779089",
   "metadata": {},
   "source": [
    "\n",
    "### Goal: Visualize the transformed data using Tableau dashboards.\n",
    "\n",
    "1. **Export Transformed Data**:\n",
    "   - Use the following code to export data from Redshift to CSV for Tableau.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e657680",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "def export_data_to_csv():\n",
    "    df = pd.read_sql(\"SELECT * FROM noaa_weather\", con=your_redshift_connection)\n",
    "    df.to_csv(\"noaa_weather.csv\", index=False)\n",
    "    print(\"Data exported to CSV.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549cd102",
   "metadata": {},
   "source": [
    "## Phase 8: Deployment and Automation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12a22fe",
   "metadata": {},
   "source": [
    "\n",
    "### Goal: Deploy the entire pipeline and set up automation for production.\n",
    "\n",
    "Example Airflow DAG snippet for automating the pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4ae1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from airflow import DAG\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "from datetime import datetime\n",
    "\n",
    "with DAG('noaa_pipeline', start_date=datetime(2024, 10, 17), schedule_interval='@daily') as dag:\n",
    "    extract_task = PythonOperator(task_id='extract_data', python_callable=extract_data_from_s3)\n",
    "    validate_task = PythonOperator(task_id='validate_data', python_callable=validate_data)\n",
    "    extract_task >> validate_task\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af75a2f",
   "metadata": {},
   "source": [
    "## Phase 9: Testing, Optimization, and Future Planning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021790f8",
   "metadata": {},
   "source": [
    "\n",
    "### Goal: Test the entire pipeline and plan for future extensions.\n",
    "\n",
    "1. **Test End-to-End Execution**: Verify that each step works as expected.\n",
    "2. **Optimize Performance**: Partition datasets and tune queries.\n",
    "3. **Explore Databricks**: Consider Databricks for future ML-based projects.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9cd480",
   "metadata": {},
   "source": [
    "1. NOAA PMN (Phytoplankton Monitoring Network) Dataset\n",
    "API Access (ERDDAP Server):\n",
    "PMN Dataset (in JSON):\n",
    "https://coastwatch.pfeg.noaa.gov/erddap/tabledap/noaa_pmn.json\n",
    "\n",
    "PMN Dataset Metadata and Parameter Documentation:\n",
    "https://coastwatch.pfeg.noaa.gov/erddap/tabledap/noaa_pmn.html\n",
    "\n",
    "Description of Key Parameters:\n",
    "time: The timestamp of the observation.\n",
    "latitude / longitude: Geographic location of the sample.\n",
    "chlorophyll: Chlorophyll concentration in the water (mg/m³).\n",
    "temperature: Sea surface temperature at the time of observation (°C).\n",
    "salinity: Salinity levels (PSU - Practical Salinity Units).\n",
    "2. NOAA Buoy Data (NDBC)\n",
    "API Access:\n",
    "Station Data (JSON):\n",
    "https://www.ndbc.noaa.gov/data/realtime2/\n",
    "Example: https://www.ndbc.noaa.gov/data/realtime2/41009.txt\n",
    "\n",
    "Buoy Metadata and Documentation:\n",
    "https://www.ndbc.noaa.gov/measdes.shtml\n",
    "\n",
    "Description of Key Parameters:\n",
    "YY / MM / DD / hh / mm: Year, month, day, hour, and minute of observation.\n",
    "WDIR: Wind direction (° from true north).\n",
    "WSPD: Wind speed (m/s).\n",
    "GST: Wind gust (m/s).\n",
    "WVHT: Wave height (meters).\n",
    "DPD: Dominant wave period (seconds).\n",
    "APD: Average wave period (seconds).\n",
    "PRES: Atmospheric pressure at sea level (hPa).\n",
    "ATMP: Air temperature (°C).\n",
    "WTMP: Sea surface temperature (°C).\n",
    "3. NOAA Climate Data (NCDC)\n",
    "API Access:\n",
    "NOAA Climate Data Online (CDO) API:\n",
    "https://www.ncdc.noaa.gov/cdo-web/webservices/v2\n",
    "API Token Registration:\n",
    "To access the NOAA CDO API, you’ll need to register for a token here:\n",
    "https://www.ncdc.noaa.gov/cdo-web/token\n",
    "\n",
    "Endpoints:\n",
    "Datasets: https://www.ncdc.noaa.gov/cdo-web/api/v2/datasets\n",
    "Locations: https://www.ncdc.noaa.gov/cdo-web/api/v2/locations\n",
    "Description of Key Parameters:\n",
    "DATE: Date of observation.\n",
    "TMAX / TMIN: Maximum and minimum temperatures (°C).\n",
    "PRCP: Precipitation (mm).\n",
    "SNOW: Snowfall (mm).\n",
    "WIND: Wind speed (m/s).\n",
    "EVAP: Evaporation (mm)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348003c1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
